# üéØ Scikit-learn (Sklearn) Cheatsheet ‚Äî Code + Bangla Explanation

---

## ‡ßß. Data Generation (‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø)

```python
from sklearn.datasets import make_classification, make_regression, make_blobs

# ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_cls, y_cls = make_classification(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
    n_features=2,            # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü‡ßá ‡ß®‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá
    n_informative=1,         # ‡ßß‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶≤‡ßá‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£
    n_redundant=0,           # ‡¶ï‡ßã‡¶®‡ßã redundant ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ (‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶ï‡¶™‡¶ø)
    n_classes=2,             # ‡ß®‡¶ü‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ (‡¶Ø‡ßá‡¶Æ‡¶®: ‡ß¶ ‡¶è‡¶¨‡¶Ç ‡ßß)
    n_clusters_per_class=1,  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡ßß‡¶ü‡¶æ cluster ‡¶•‡¶æ‡¶ï‡¶¨‡ßá (group)
    class_sep=2.0,           # ‡¶¶‡ßÅ‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨ ‡¶ï‡¶§ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá (‡¶¨‡ßú ‡¶π‡¶≤‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶π‡¶ú)
    random_state=42          # ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶Æ ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ, ‡¶Ø‡¶æ‡¶§‡ßá ‡¶è‡¶ï‡¶á ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶∞ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü
)

# ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_reg, y_reg = make_regression(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü
    n_features=1,            # ‡ßß‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá
    noise=10,                # ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ noise ‡¶¨‡¶æ randomness ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá, ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡¶Æ‡ßÅ‡¶ñ‡ßÄ ‡¶ï‡¶∞‡¶§‡ßá
    random_state=42          # ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶ø‡¶â‡¶∏‡¶ø‡¶¨‡¶ø‡¶≤‡¶ø‡¶ü‡¶ø ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ
)

# ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_clust, y_clust = make_blobs(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü
    centers=3,               # ‡ß©‡¶ü‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶¨‡¶æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
    n_features=2,            # ‡ß®‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ‡¶§‡ßá
    cluster_std=1.0,         # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ spread ‡¶¨‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞
    random_state=42          # ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶ø‡¶â‡¶∏‡¶ø‡¶¨‡¶ø‡¶≤‡¶ø‡¶ü‡¶ø ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ
)

```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
>
> * `make_classification`: supervised ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§
> * `make_regression`: supervised ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§
> * `make_blobs`: unsupervised ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§

---

## ‡ß®. Data Splitting (‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ)

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_cls, y_cls, test_size=0.2, random_state=42
)
```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
> ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç (‡ßÆ‡ß¶%) ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶∏‡ßç‡¶ü (‡ß®‡ß¶%) ‡¶è ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá, ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§

---

## ‡ß©. Preprocessing (‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡¶ø)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer

# ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç (mean=0, std=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç (categorical to number)
le = LabelEncoder()
y_encoded = le.fit_transform(y_train)

# One Hot Encoding (categorical to binary columns)
encoder = OneHotEncoder(sparse=False)
X_onehot = encoder.fit_transform(X_train.reshape(-1, 1))

# ‡¶Æ‡¶ø‡¶∏‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßÇ‡¶∞‡¶£
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_train)

# ‡¶™‡¶≤‡¶ø‡¶®‡ßã‡¶Æ‡¶ø‡ßü‡¶æ‡¶≤ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)
```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
> ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶â‡¶™‡¶Ø‡ßã‡¶ó‡ßÄ ‡¶ï‡¶∞‡ßá ‡¶§‡ßã‡¶≤‡¶æ ‡¶π‡ßü‡•§ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç, ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç, ‡¶Æ‡¶ø‡¶∏‡¶ø‡¶Ç ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶ï‡¶∞‡ßá‡•§

---

## ‡ß™. ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ì ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã

### Classification (‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### Regression (‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤)

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

## ‡ß´. Model Evaluation (‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á)

### Classification Metrics

```python
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_pred))
```

### Regression Metrics

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("MSE:", mean_squared_error(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))
```

---

## ‡ß¨. Cross-validation ‡¶ì Hyperparameter Tuning

```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# Cross validation
scores = cross_val_score(model, X_train, y_train, cv=5)
print("Cross-validation scores:", scores)

# Grid Search for hyperparameter tuning
params = {'C': [0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid=params, cv=5)
grid.fit(X_train, y_train)
print("Best params:", grid.best_params_)
```

---

## ‡ß≠. Pipelines (‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ)

```python
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
```

---

## ‡ßÆ. Feature Selection (‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®)

```python
from sklearn.feature_selection import SelectKBest, chi2, RFE

selector = SelectKBest(score_func=chi2, k=5)
X_new = selector.fit_transform(X_train, y_train)

# Recursive Feature Elimination
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

model = LogisticRegression()
rfe = RFE(model, n_features_to_select=5)
X_rfe = rfe.fit_transform(X_train, y_train)
```

---

## ‡ßØ. Clustering (‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç)

```python
from sklearn.cluster import KMeans, DBSCAN

kmeans = KMeans(n_clusters=3)
kmeans.fit(X_train)
labels = kmeans.labels_

dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X_train)
labels_db = dbscan.labels_
```

---

## ‡ßß‡ß¶. Dimensionality Reduction (‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßç‡¶∞‡¶æ‡¶∏)

```python
from sklearn.decomposition import PCA, TruncatedSVD

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train)

svd = TruncatedSVD(n_components=2)
X_svd = svd.fit_transform(X_train)
```

---

## ‡ßß‡ßß. Text Feature Extraction (‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞)

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

texts = ["machine learning is fun", "sklearn is powerful"]

cv = CountVectorizer()
X_counts = cv.fit_transform(texts)

tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(texts)
```

---

## ‡ßß‡ß®. Model Saving and Loading (‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ì ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞)

```python
import joblib

joblib.dump(model, 'model.pkl')
model = joblib.load('model.pkl')
```

---

## ‡ßß‡ß©. Utility Functions (‡¶∏‡¶π‡¶ú ‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®)

```python
model.get_params()      # ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡ßá‡¶ü‡¶ø‡¶Ç‡¶∏ ‡¶¶‡ßá‡¶ñ‡ßã
model.set_params()      # ‡¶®‡¶§‡ßÅ‡¶® ‡¶∏‡ßá‡¶ü‡¶ø‡¶Ç‡¶∏ ‡¶¶‡¶æ‡¶ì
model.score(X_test, y_test)  # Accuracy ‡¶¨‡¶æ R2 ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶™‡¶æ‡¶ì
```

---

