# üéØ Scikit-learn (Sklearn) Cheatsheet ‚Äî Code + Bangla Explanation

---

## ‡ßß. Data Generation (‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø)

```python
from sklearn.datasets import make_classification, make_regression, make_blobs

# ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_cls, y_cls = make_classification(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
    n_features=2,            # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü‡ßá ‡ß®‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá
    n_informative=1,         # ‡ßß‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶≤‡ßá‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£
    n_redundant=0,           # ‡¶ï‡ßã‡¶®‡ßã redundant ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ (‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶ï‡¶™‡¶ø)
    n_classes=2,             # ‡ß®‡¶ü‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ (‡¶Ø‡ßá‡¶Æ‡¶®: ‡ß¶ ‡¶è‡¶¨‡¶Ç ‡ßß)
    n_clusters_per_class=1,  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡ßß‡¶ü‡¶æ cluster ‡¶•‡¶æ‡¶ï‡¶¨‡ßá (group)
    class_sep=2.0,           # ‡¶¶‡ßÅ‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨ ‡¶ï‡¶§ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá (‡¶¨‡ßú ‡¶π‡¶≤‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶π‡¶ú)
    random_state=42          # ‡¶∞‚Äç‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶Æ ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ, ‡¶Ø‡¶æ‡¶§‡ßá ‡¶è‡¶ï‡¶á ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶∞ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü
)

# ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_reg, y_reg = make_regression(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü
    n_features=1,            # ‡ßß‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá
    noise=10,                # ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ noise ‡¶¨‡¶æ randomness ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá, ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡¶Æ‡ßÅ‡¶ñ‡ßÄ ‡¶ï‡¶∞‡¶§‡ßá
    random_state=42          # ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶ø‡¶â‡¶∏‡¶ø‡¶¨‡¶ø‡¶≤‡¶ø‡¶ü‡¶ø ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ
)

# ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
X_clust, y_clust = make_blobs(
    n_samples=100,           # ‡¶Æ‡ßã‡¶ü ‡ßß‡ß¶‡ß¶‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü
    centers=3,               # ‡ß©‡¶ü‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶¨‡¶æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
    n_features=2,            # ‡ß®‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ‡¶§‡ßá
    cluster_std=1.0,         # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡¶∞‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ spread ‡¶¨‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞
    random_state=42          # ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶ø‡¶â‡¶∏‡¶ø‡¶¨‡¶ø‡¶≤‡¶ø‡¶ü‡¶ø ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶ø‡¶° ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ
)

```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
>
> * `make_classification`: supervised ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§
> * `make_regression`: supervised ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§
> * `make_blobs`: unsupervised ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§

---

## ‡ß®. Data Splitting (‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ)

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_cls, y_cls, test_size=0.2, random_state=42
)
```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
> ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç (‡ßÆ‡ß¶%) ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶∏‡ßç‡¶ü (‡ß®‡ß¶%) ‡¶è ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá, ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§

---

## ‡ß©. Preprocessing (‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡¶ø)
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer

scaler = StandardScaler()                      # StandardScaler ‡¶¶‡¶ø‡ßü‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç ‡¶ï‡¶∞‡¶æ (Mean=0, Std=1)
X_scaled = scaler.fit_transform(X_train)      # fit_transform ‡¶ï‡¶∞‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ mean/std ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç ‡¶ï‡¶∞‡ßá

le = LabelEncoder()                            # LabelEncoder ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶ï‡¶æ‡¶≤ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡¶ï‡ßá ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ
y_encoded = le.fit_transform(y_train)         # fit_transform ‡¶¶‡¶ø‡ßü‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡¶ï‡ßá unique ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶¶‡ßá‡ßü

encoder = OneHotEncoder(sparse=False)          # OneHotEncoder ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶ï‡¶æ‡¶≤ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞‡¶ï‡ßá ‡¶¨‡¶æ‡¶á‡¶®‡¶æ‡¶∞‡¶ø ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ
X_onehot = encoder.fit_transform(X_train.reshape(-1, 1))  # reshape(-1,1) ‡¶¶‡¶ø‡ßü‡ßá 1D ‡¶ï‡ßá 2D ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã ‡¶π‡ßü

imputer = SimpleImputer(strategy='mean')      # SimpleImputer ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶ø‡¶∏‡¶ø‡¶Ç ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ (NaN) ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ
X_imputed = imputer.fit_transform(X_train)    # ‡¶Æ‡¶ø‡¶∏‡¶ø‡¶Ç ‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡ßü ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶ó‡ßú ‡¶Æ‡¶æ‡¶® ‡¶¨‡¶∏‡¶æ‡¶®‡ßã ‡¶π‡ßü

poly = PolynomialFeatures(degree=2)           # PolynomialFeatures ‡¶¶‡¶ø‡ßü‡ßá degree=2 ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø
X_poly = poly.fit_transform(X_train)           # ‡¶Ø‡ßá‡¶Æ‡¶®: x¬≤, x*y ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø interaction terms ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü
```

> **‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
> ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶â‡¶™‡¶Ø‡ßã‡¶ó‡ßÄ ‡¶ï‡¶∞‡ßá ‡¶§‡ßã‡¶≤‡¶æ ‡¶π‡ßü‡•§ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç, ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç, ‡¶Æ‡¶ø‡¶∏‡¶ø‡¶Ç ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶ï‡¶∞‡ßá‡•§

---

## ‡ß™. ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ì ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã

### Classification (‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤)

```python
from sklearn.linear_model import LogisticRegression          # Logistic Regression ‡¶Æ‡¶°‡ßá‡¶≤
from sklearn.tree import DecisionTreeClassifier               # Decision Tree Classifier ‡¶Æ‡¶°‡ßá‡¶≤
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Ensemble ‡¶Æ‡¶°‡ßá‡¶≤
from sklearn.svm import SVC                                   # Support Vector Classifier (SVM)

model = LogisticRegression()                                 # Logistic Regression ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶§‡ßà‡¶∞‡¶ø
model.fit(X_train, y_train)                                  # ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã
y_pred = model.predict(X_test)                               # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶ø‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ

```

### Regression (‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤)

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso                 # Linear, Ridge, Lasso regression ‡¶Æ‡¶°‡ßá‡¶≤
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor   # Ensemble regression ‡¶Æ‡¶°‡ßá‡¶≤

model = LinearRegression()                               # Linear Regression ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶§‡ßà‡¶∞‡¶ø
model.fit(X_train, y_train)                              # ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã
y_pred = model.predict(X_test)                           # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶ø‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ

```

---

## ‡ß´. Model Evaluation (‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á)

### Classification Metrics

```python
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score

print("Accuracy:", accuracy_score(y_test, y_pred))                     # ‡¶∏‡¶†‡¶ø‡¶ï ‡¶≠‡¶¨‡¶ø‡¶∑‡ßç‡¶Ø‡¶§‡ßá‡¶∞ ‡¶π‡¶æ‡¶∞ (Accuracy)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))        # ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶≠‡ßÅ‡¶≤-‡¶∏‡¶†‡¶ø‡¶ï ‡¶≠‡¶¨‡¶ø‡¶∑‡ßç‡¶Ø‡¶§‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏
print("Classification Report:\n", classification_report(y_test, y_pred))  # Precision, Recall, F1-score ‡¶è‡¶∞ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü
print("F1 Score:", f1_score(y_test, y_pred))                           # Precision ‡¶ì Recall ‡¶è‡¶∞ ‡¶π‡¶∞‡¶Æ‡ßã‡¶®‡¶ø‡¶ï ‡¶Æ‡ßÄ‡¶®
print("ROC AUC Score:", roc_auc_score(y_test, y_pred))                 # ROC curve ‡¶è‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶è‡¶≤‡¶æ‡¶ï‡¶æ, ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶Æ‡¶æ‡¶™‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶®‡¶¶‡¶£‡ßç‡¶°

```

### Regression Metrics

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("MSE:", mean_squared_error(y_test, y_pred))      # Mean Squared Error (‡¶ó‡ßú ‡¶¨‡¶∞‡ßç‡¶ó‡¶Æ‡ßÇ‡¶≤ ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø), ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶Æ‡¶æ‡¶™‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
print("MAE:", mean_absolute_error(y_test, y_pred))     # Mean Absolute Error (‡¶ó‡ßú ‡¶∏‡¶†‡¶ø‡¶ï ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø), ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø‡¶∞ ‡¶ó‡ßú ‡¶Æ‡¶æ‡¶™
print("R2 Score:", r2_score(y_test, y_pred))            # R¬≤ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞, ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ (‡ßß=‡¶™‡¶æ‡¶∞‡¶´‡ßá‡¶ï‡ßç‡¶ü, ‡ß¶=‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™)

```

---

## ‡ß¨. Cross-validation ‡¶ì Hyperparameter Tuning

```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# Cross validation ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ
scores = cross_val_score(model, X_train, y_train, cv=5)    # ‡ß´ ‡¶≠‡¶æ‡¶ó‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡ßá
print("Cross-validation scores:", scores)                 # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá

# Grid Search ‡¶¶‡¶ø‡ßü‡ßá Hyperparameter Tuning ‡¶ï‡¶∞‡¶æ
params = {'C': [0.1, 1, 10]}                              # Logistic Regression ‡¶è‡¶∞ C ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
grid = GridSearchCV(LogisticRegression(), param_grid=params, cv=5)  # ‡ß´ ‡¶´‡ßã‡¶≤‡ßç‡¶° ‡¶ï‡ßç‡¶∞‡¶∏-‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶®‡ßá ‡¶¨‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
grid.fit(X_train, y_train)                                 # ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ
print("Best params:", grid.best_params_)                  # ‡¶∏‡ßá‡¶∞‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá

```

---

## ‡ß≠. Pipelines (‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ)

```python
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),           # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶ø‡¶Ç ‡¶ï‡¶∞‡¶¨‡ßá (Mean=0, Std=1)
    ('model', LogisticRegression())         # ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ Logistic Regression ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶¨‡ßá
])

pipe.fit(X_train, y_train)                   # Pipeline ‡¶™‡ßÅ‡¶∞‡ßã ‡¶™‡ßç‡¶∞‡ßã‡¶∏‡ßá‡¶∏ ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç
y_pred = pipe.predict(X_test)                # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡¶ø‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶¨‡ßá

```

---

## ‡ßÆ. Feature Selection (‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®)

```python
from sklearn.feature_selection import SelectKBest, chi2, RFE

selector = SelectKBest(score_func=chi2, k=5)      # Chi-square ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡ßá‡¶∞‡¶æ ‡ß´‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶¨‡ßá‡¶õ‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ
X_new = selector.fit_transform(X_train, y_train) # ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡¶∂‡¶®‡ßá‡¶∞ ‡¶™‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü

# Recursive Feature Elimination (RFE) - ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡¶¨‡ßÉ‡¶§‡ßç‡¶§‡¶ø‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡ßü‡¶æ
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

model = LogisticRegression()                       # ‡¶¨‡ßá‡¶∏ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá Logistic Regression
rfe = RFE(model, n_features_to_select=5)          # ‡ß´‡¶ü‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶õ‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø RFE ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü
X_rfe = rfe.fit_transform(X_train, y_train)       # ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶§‡ßà‡¶∞‡¶ø

```

---

## ‡ßØ. Clustering (‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç)

```python
from sklearn.cluster import KMeans, DBSCAN

kmeans = KMeans(n_clusters=3)         # KMeans ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç, ‡ß©‡¶ü‡¶æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨‡ßá
kmeans.fit(X_train)                   # ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã
labels = kmeans.labels_               # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ (0,1,2)

dbscan = DBSCAN(eps=0.5, min_samples=5)  # DBSCAN ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç, eps=0.5 (‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨), min_samples=5 (‡¶®‡ßÇ‡¶®‡ßç‡¶Ø‡¶§‡¶Æ ‡¶™‡ßü‡ßá‡¶®‡ßç‡¶ü)
dbscan.fit(X_train)                      # ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã
labels_db = dbscan.labels_               # ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ (-1 ‡¶Æ‡¶æ‡¶®‡ßá noise/outlier)

```

---

## ‡ßß‡ß¶. Dimensionality Reduction (‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßç‡¶∞‡¶æ‡¶∏)

```python
‡¶†‡¶ø‡¶ï ‡¶≠‡¶æ‡¶á, ‡¶°‡¶æ‡¶® ‡¶™‡¶æ‡¶∂‡ßá ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶π ‡¶ï‡ßã‡¶°‡¶ü‡¶æ ‡¶¶‡¶ø‡¶≤‡¶æ‡¶Æ:

```python
from sklearn.decomposition import PCA, TruncatedSVD

pca = PCA(n_components=2)                  # PCA ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßç‡¶∞‡¶æ‡¶∏ (Dimensionality Reduction), ‡ß®‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶ï‡¶Æ‡ßç‡¶™‡ßã‡¶®‡ßá‡¶®‡ßç‡¶ü ‡¶®‡ßá‡¶ì‡ßü‡¶æ
X_pca = pca.fit_transform(X_train)        # ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶§‡ßà‡¶∞‡¶ø

svd = TruncatedSVD(n_components=2)        # TruncatedSVD, PCA ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßç‡¶∞‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Ü‡¶∞‡ßá‡¶ï ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø, ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡¶∞‡ßá sparse ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡¶æ‡¶≤‡ßã
X_svd = svd.fit_transform(X_train)        # ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶ï‡¶Æ‡¶ø‡ßü‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø
```

---

## ‡ßß‡ßß. Text Feature Extraction (‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞)

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

texts = ["machine learning is fun", "sklearn is powerful"]   # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤

cv = CountVectorizer()                                       # CountVectorizer ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ó‡¶£‡¶®‡¶æ (‡¶¨‡¶ó ‡¶Ö‡¶´ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡¶∏)
X_counts = cv.fit_transform(texts)                           # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ

tfidf = TfidfVectorizer()                                    # TfidfVectorizer ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨ (TF-IDF) ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡ßá
X_tfidf = tfidf.fit_transform(texts)                         # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá TF-IDF ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ

```

---

## ‡ßß‡ß®. Model Saving and Loading (‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ì ‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞)

```python
import joblib

joblib.dump(model, 'model.pkl')      # ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá 'model.pkl' ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡¶æ (save)
model = joblib.load('model.pkl')     # ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶ø‡¶§ ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ (load) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

```

---

## ‡ßß‡ß©. Utility Functions (‡¶∏‡¶π‡¶ú ‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®)

```python
model.get_params()                   # ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶ì ‡¶∏‡ßá‡¶ü‡¶ø‡¶Ç‡¶∏ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
model.set_params(**new_params)      # ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Ø‡ßá‡¶Æ‡¶® set_params(C=1.0)
model.score(X_test, y_test)          # ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶™‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Ø‡ßá‡¶Æ‡¶® accuracy ‡¶¨‡¶æ R¬≤ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞

# set_params()-‡¶è ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ dictionary ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá, ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

model.set_params(C=0.5, max_iter=200)
```

---

